# -*- coding: utf-8 -*-
"""DiabetesPredictionProjectCode-11600006-SANTHOSHIKAREDDY

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1X5sVpJv4emZ63l1IsbP5FIHNLf9qATwZ
"""

#Let's start with importing necessary libraries

import pandas as pd 
import numpy as np 
from sklearn.preprocessing import StandardScaler 
from sklearn.linear_model  import  LogisticRegression
from sklearn.model_selection import train_test_split
from statsmodels.stats.outliers_influence import variance_inflation_factor 
from sklearn.metrics import accuracy_score, confusion_matrix, roc_curve, roc_auc_score
import matplotlib.pyplot as plt
import seaborn as sns

df = pd.read_csv("/content/diabetes.csv") # Reading the dff.head()

df

df.describe()

# let's see how df is distributed for every column
plt.figure(figsize=(20,25), facecolor='white')
plotnumber = 1

for column in df:
    if plotnumber<=9 :     # as there are 9 columns in the df
        ax = plt.subplot(3,3,plotnumber)
        sns.distplot(df[column])
        plt.xlabel(column,fontsize=20)
        #plt.ylabel('Salary',fontsize=20)
    plotnumber+=1
plt.show()

# replacing zero values with the mean of the column
df['BMI'] = df['BMI'].replace(0,df['BMI'].mean())
df['BloodPressure'] = df['BloodPressure'].replace(0,df['BloodPressure'].mean())
df['Glucose'] = df['Glucose'].replace(0,df['Glucose'].mean())
df['Insulin'] = df['Insulin'].replace(0,df['Insulin'].mean())
df['SkinThickness'] = df['SkinThickness'].replace(0,df['SkinThickness'].mean())

fig, ax = plt.subplots(figsize=(15,10))
sns.boxplot(data=df, width= 0.5,ax=ax,  fliersize=3)

q = df['Insulin'].quantile(.70)
df_new = df[df['Insulin'] < q]

df.corr

santhu = df.corr(method = "spearman")
plt.figure(figsize=(20,20))
#plot heat map
g=sns.heatmap(santhu,annot=True)

X = df.drop(columns = ['Outcome'])
y = df['Outcome']

# Set the target variable
target_var = "Outcome"

# Set the features to use
feature_cols = df.columns.drop(target_var)

# Create the strip plot
plt.figure(figsize=(20,25), facecolor='white')
plotnumber = 1

for column in feature_cols:
    if plotnumber<=9 :
        ax = plt.subplot(3,3,plotnumber)
        sns.stripplot(x=df[target_var], y=df[column])
        plotnumber+=1
        
plt.tight_layout()
plt.show()

scalar = StandardScaler()
X_scaled = scalar.fit_transform(X)

vif = pd.DataFrame()
vif["vif"] = [variance_inflation_factor(X_scaled,i) for i in range(X_scaled.shape[1])]
vif["Features"] = X.columns

#let's check the values
vif

x_train,x_test,y_train,y_test = train_test_split(X_scaled,y, test_size= 0.25, random_state = 355)

log_reg = LogisticRegression()

log_reg.fit(x_train,y_train)

y_pred = log_reg.predict(x_test)

accuracy = accuracy_score(y_test,y_pred)
accuracy

# Confusion Matrix
conf_mat = confusion_matrix(y_test,y_pred)
conf_mat

true_positive = conf_mat[0][0]
false_positive = conf_mat[0][1]
false_negative = conf_mat[1][0]
true_negative = conf_mat[1][1]

# Breaking down the formula for Accuracy
Accuracy = (true_positive + true_negative) / (true_positive +false_positive + false_negative + true_negative)
Accuracy

# Precision
Precision = true_positive/(true_positive+false_positive)
Precision

# Recall
Recall = true_positive/(true_positive+false_negative)
Recall

# F1 Score
F1_Score = 2*(Recall * Precision) / (Recall + Precision)
F1_Score

# Area Under Curve
auc = roc_auc_score(y_test, y_pred)
auc

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

plt.plot(fpr, tpr, color='orange', label='ROC')
plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)' % auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve')
plt.legend()
plt.show()

from sklearn.naive_bayes import GaussianNB
model = GaussianNB()

model.fit(x_train,y_train)

y_pred = model.predict(x_test)

accuracynb = accuracy_score(y_test,y_pred)
accuracynb

# Confusion Matrix
conf_mat = confusion_matrix(y_test,y_pred)
conf_mat

fpr, tpr, thresholds = roc_curve(y_test, y_pred)

plt.plot(fpr, tpr, color='orange', label='ROC')
plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--',label='ROC curve (area = %0.2f)' % auc)
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic (ROC) Curve for Naive Bayes')
plt.legend()
plt.show()

# let's fit the data into kNN model and see how well it performs:
from sklearn.neighbors import KNeighborsClassifier
#Let's start with importing necessary libraries
from sklearn.preprocessing import StandardScaler 
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.model_selection import KFold
knn = KNeighborsClassifier()
knn.fit(x_train,y_train)

knn.score(x_train,y_train)

knn.score(x_test,y_test)

#k-fold cross validation 
kfold = KFold(n_splits=12)
kfold.get_n_splits(X_scaled)

from statistics import mean
knn = KNeighborsClassifier(algorithm = 'ball_tree', leaf_size =18, n_neighbors =11)
cnt =0
count=[]
train_score =[]
test_score = []

for train_index,test_index in kfold.split(X_scaled):
    X_train, X_test = X_scaled[train_index], X_scaled[test_index] # our scaled data is an array so it can work on x[value]
    y_train, y_test = y.iloc[train_index], y.iloc[test_index] # y is a dataframe so we have to use "iloc" to retreive data
    knn.fit(X_train,y_train)
    train_score_ = knn.score(X_train,y_train)
    test_score_ =  knn.score(X_test,y_test)
    cnt+=1
    count.append(cnt)
    train_score.append(train_score_)
    test_score.append(test_score_)
    
    print("for k = ", cnt)
    print("train_score is :  ", train_score_, "and test score is :  ", test_score_)
print("************************************************")
print("************************************************")
print("Average train score is :  ", mean(train_score))
print("Average test score is :  ", mean(test_score))

# let's plot the test_accuracy with the value of k in k-fold

plt.plot(count,test_score)
plt.xlabel('Value of K for k-fold')
plt.ylabel('test accuracy')
plt.xticks(np.arange(0, 12, 1)) 
plt.yticks(np.arange(0.65, 1, 0.05))

from sklearn.tree import DecisionTreeClassifier

dtree = DecisionTreeClassifier()
dtree.fit(X_train, y_train)

y_pred = dtree.predict(X_test)

dtree_train_acc = accuracy_score(y_train, dtree.predict(X_train))
dtree_test_acc = accuracy_score(y_test, y_pred)

print(f"Training Accuracy of Decision Tree Model is {dtree_train_acc}")
print(f"Test Accuracy of Decision Tree Model is {dtree_test_acc}")

confusion_matrix(y_test, y_pred)

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
print(classification_report(y_test, y_pred))

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

param_grid = {'n_estimators': [500, 600,700], 
              'max_leaf_nodes': [60,90]}
random_forest = RandomForestClassifier(n_jobs=-1)

gridRF = GridSearchCV(random_forest, 
                              param_grid=param_grid, 
                              cv=3,
                              scoring='recall',
                              return_train_score=True)
gridRF.fit(X_train, y_train)
gridRF.best_params_

print(classification_report(y_test, gridRF.predict(X_test)))

from keras.models import Sequential
from keras.layers import Dense

from keras.utils import np_utils
y_train = np_utils.to_categorical(y_train)

#Defining and desining neural network model
model = Sequential()
model.add(Dense(12, input_dim=8, activation='relu'))
model.add(Dense(8, activation='relu'))
model.add(Dense(2, activation='sigmoid'))

# Compiling model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

model.fit(X_train, y_train, epochs=50, batch_size=10)

#prediction and evalution
from sklearn.metrics import accuracy_score
y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)
accuracy_score(y_test, y_pred)